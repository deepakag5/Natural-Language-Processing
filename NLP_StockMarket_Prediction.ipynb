{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility libraries\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import calendar\n",
    "import warnings\n",
    "\n",
    "# Prepocessing libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras imports\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Bidirectional, Embedding, LSTM, Dense, Conv1D, GlobalMaxPool1D, MaxPool1D, MaxPooling1D, Dropout, Activation , Flatten , Input, concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersFile = 'news_reuters.csv'\n",
    "stockFile = 'stockReturns.json'\n",
    "\n",
    "rawX = pd.read_csv('news_reuters.csv', header=None, \n",
    "                   names=['ticker', 'company', 'pub_date', 'headline', 'first_sent', 'category'])\n",
    "rawY = pd.read_json('stockReturns.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_y_data(data, tickerType='mid'):\n",
    "    \"\"\"Convert stock data into binary postive/negative\"\"\"\n",
    "    tmp = data[tickerType].apply(pd.Series)\n",
    "    tmp = tmp.stack().rename('price', inplace=True).reset_index()\n",
    "    tmp['y'] = np.where(tmp['price'] >= 0, 1, 0)\n",
    "    tmp.rename(columns={'level_0': 'ticker', 'level_1': 'pub_date'}, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def clean_and_merge_data(X, Y):\n",
    "    \"\"\"Filter X to only those tickers with stock data\"\"\"\n",
    "    y_tickers = set(Y['ticker'])\n",
    "    X = X.loc[X['ticker'].isin(y_tickers)]\n",
    "    # Make sure data types are the same for merge    \n",
    "    Y['pub_date'] = Y['pub_date'].astype(rawX['pub_date'].dtype)\n",
    "    Y['ticker'] = Y['ticker'].astype(rawX['ticker'].dtype)\n",
    "    return X.merge(Y, on=['ticker', 'pub_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanY = reformat_y_data(rawY, 'short')\n",
    "merged = clean_and_merge_data(rawX, cleanY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up text columns and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    \"\"\"Clean up text data by:\n",
    "    \n",
    "    1. Replacing double spaces into a single space\n",
    "    2. Replace U.S. to United States so U won't get deleted with next \n",
    "       replacement\n",
    "    3. Remove all capitalized words at the beginning of the \n",
    "       sentence, since those are mostly places (aka NEW YORK)\n",
    "    4. Remove unnecessary punctuation (hyphens and asterisks)\n",
    "    5. Remove dates\n",
    "    \"\"\"\n",
    "    monthStrings = list(calendar.month_name)[1:] + list(calendar.month_abbr)[1:]\n",
    "    monthPattern = '|'.join(monthStrings)\n",
    "    \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    sent = re.sub(r'U.S.', 'United States', sent)\n",
    "    sent = re.sub(r'^(\\W?[A-Z\\s\\d]+\\b-?)', '', sent)\n",
    "    sent = re.sub(r'^ ?\\W ', '', sent)\n",
    "    sent = re.sub(r'({}) \\d+'.format(monthPattern), '', sent)\n",
    "    \n",
    "    # replace double spaces one more time after previous cleaning \n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    return sent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(col):\n",
    "    \"\"\"Tokenize string into a sequence of words\"\"\"\n",
    "    return [text_to_word_sequence(text, lower=False) for text in col]\n",
    "\n",
    "def filt_to_one(x, random_state=10):\n",
    "    \"\"\"Filter dataset so that there is only one observation per day.\n",
    "    \n",
    "    If there is more than one record, will use the topStory record\n",
    "    if one exists.  If one doesn't or there are 2 topStory records\n",
    "    then it will randomly select one of the observations.\n",
    "    \"\"\"\n",
    "    if x.shape[0] > 1:\n",
    "        if 'topStory' in x['category'].unique():\n",
    "            x = x.loc[x['category'] == 'topStory']\n",
    "        if x.shape[0] > 1:\n",
    "            x = x.sample(n=1, random_state=random_state)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up text\n",
    "merged['headline'] = merged.headline.apply(clean_text)\n",
    "merged['first_sent'] = merged.first_sent.apply(clean_text)\n",
    "\n",
    "# Turn sentences into tokens\n",
    "merged['headline_token'] = tokenize_sent(merged.headline)\n",
    "merged['first_sent_token'] = tokenize_sent(merged.first_sent)\n",
    "\n",
    "\n",
    "# Get one record per company/day\n",
    "finalData = merged.groupby(by=['ticker', 'pub_date']).apply(filt_to_one)\n",
    "\n",
    "# Combine Headline and First Sentence into one text \n",
    "finalData['final_text'] = finalData['headline_token'] + finalData.first_sent_token\n",
    "\n",
    "# Remove observations with missing stock price\n",
    "finalData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['ticker2', 'company', 'pub_date2', 'headline', 'first_sent', 'category', 'price', 'y', 'headline_token', 'first_sent_token', 'final_text']\n",
    "finalData.columns = new_cols\n",
    "finalData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = finalData['headline'].values\n",
    "y = finalData['y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a train and test set, retaining the same test set for every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (8542, 100)\n",
      "Shape of label tensor: (8542, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=200)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_data = pad_sequences(sequences, maxlen=100)\n",
    "###################################\n",
    "MAX_NUM_WORDS=40 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH=100 # max number of words in a review to use\n",
    "word_index = tokenizer.word_index\n",
    "y_train_labels = to_categorical(np.asarray(y_train))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', y_train_labels.shape)\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2848, 100)\n",
      "Shape of label tensor: (2848, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_test = Tokenizer(num_words=200)\n",
    "tokenizer_test.fit_on_texts(X_test)\n",
    "sequences_test = tokenizer_test.texts_to_sequences(X_test)\n",
    "test_data = pad_sequences(sequences_test, maxlen=100)\n",
    "###################################\n",
    "MAX_NUM_WORDS=40 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH=100 # max number of words in a review to use\n",
    "word_index_text = tokenizer_test.word_index\n",
    "y_test_labels = to_categorical(np.asarray(y_test))\n",
    "print('Shape of data tensor:', test_data.shape)\n",
    "print('Shape of label tensor:', y_test_labels.shape)\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR=''\n",
    "\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100 # how big is each word vector\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "       # words not found in embedding index will be all-zeros.\n",
    "       embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(seq_input_len, embed_matrix, \n",
    "                     n_RNN_nodes, n_dense_nodes, \n",
    "                     recurrent_dropout=0.2, \n",
    "                     drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(seq_input_len,), name='word_input_layer')\n",
    "        \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    hidden_layer1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    \n",
    "    hidden_layer2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(hidden_layer1)\n",
    "\n",
    "    dense_layer = Dense(units=n_dense_nodes, activation='relu', name='dense_layer')(hidden_layer2)\n",
    "\n",
    "    drop_out3 = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(units=n_out, activation='softmax',\n",
    "                         name='output_layer')(drop_out3)\n",
    "\n",
    "    model = Model(inputs=[word_input], outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2\n",
    "nb_epoch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to calculate precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = create_rnn_model(seq_input_len=train_data.shape[-1],\n",
    "                             embed_matrix=embedding_matrix, \n",
    "                             recurrent_dropout=.4, drop_out=.5,\n",
    "                             n_RNN_nodes=500, n_dense_nodes=500, n_out=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(model, x_train, y_train, x_test, y_test, \n",
    "                         modelSaveName, modelSavePath='',\n",
    "                         batch_size=128, epochs=2, validation_split=.1):\n",
    "\n",
    "    \"\"\"Train model, save weights, and predict data\"\"\"\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    filepath = os.path.join(modelSavePath, modelSaveName + '.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(x=x_train, y=y_train, batch_size=batch_size, \n",
    "              epochs=epochs, validation_split=validation_split, \n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    score, acc, rec, prec = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    return (model, acc, rec, prec)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Show the architecture for model (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Report the peformance according to some metric (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "word_input_layer (InputLayer (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "word_embedding_layer (Embedd (None, 100, 100)          1009700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 1000)         2404000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1000)              6004000   \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 9,919,202\n",
      "Trainable params: 9,919,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7687 samples, validate on 855 samples\n",
      "Epoch 1/1\n",
      "7687/7687 [==============================] - 56s 7ms/step - loss: 0.7045 - acc: 0.4877 - recall: 0.4877 - precision: 0.4878 - val_loss: 0.6940 - val_acc: 0.5158 - val_recall: 0.5158 - val_precision: 0.5158\n",
      "\n",
      "Epoch 00001: saving model to rnn_model.hdf5\n",
      "2848/2848 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "rnn_res = train_and_test_model(rnn_model, train_data, y_train_labels, test_data, y_test_labels, 'rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Show the architecture for model (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Report the peformance according to some metric (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 92\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "char_input_layer (InputLayer (None, 1024, 92)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1018, 128)         82560     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 203, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 199, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 33, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 6, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              787456    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 1,085,442\n",
      "Trainable params: 1,085,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7687 samples, validate on 855 samples\n",
      "Epoch 1/1\n",
      "7687/7687 [==============================] - 8s 1ms/step - loss: 0.6934 - acc: 0.4949 - recall: 0.4869 - precision: 0.4951 - val_loss: 0.6929 - val_acc: 0.5170 - val_recall: 0.5170 - val_precision: 0.5170\n",
      "\n",
      "Epoch 00001: saving model to cnn_model.hdf5\n",
      "2848/2848 [==============================] - 2s 553us/step\n"
     ]
    }
   ],
   "source": [
    "def vectorize_sentences(data, lexicon, maxlen=200):\n",
    "    X = []\n",
    "    for sentences in data:\n",
    "        x = [lexicon[token] if token in lexicon else lexicon['<UNK>'] for \n",
    "                                 token in sentences]\n",
    "        x2 = np.eye(len(char_indices) + 1)[x]\n",
    "        X.append(x2)\n",
    "    return (pad_sequences(X, maxlen=maxlen))\n",
    "\n",
    "def create_cnn_model(char_maxlen, vocab_size,\n",
    "                     nb_filter=100, filter_kernels = [4] * 4,\n",
    "                     pool_size=3, n_dense_nodes=100,\n",
    "                     drop_out=.2, n_out=2):\n",
    "\n",
    "    inputs = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "\n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(inputs)\n",
    "    \n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    flatten = Flatten()(maxpool3)\n",
    "\n",
    "    dense_layer = Dense(n_dense_nodes, activation='relu')(flatten)\n",
    "    dropout = Dropout(drop_out)(dense_layer)\n",
    "\n",
    "    output_layer = Dense(n_out, activation='softmax', name='output')(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "    return model \n",
    "\n",
    "char_maxlen = 1024 \n",
    "nb_filter = 128\n",
    "dense_outputs = 1024\n",
    "filter_kernels = [7, 5, 5, 3]\n",
    "pool_size = 5\n",
    "\n",
    "# Turn all tokens into one string and then all obs \n",
    "# into one overall string\n",
    "trainTokensAsString = X_train\n",
    "testTokensAsString = X_test\n",
    "oneTxt = ' '.join(trainTokensAsString)\n",
    "\n",
    "# Get info about characters\n",
    "chars = set(oneTxt)\n",
    "vocab_size = len(chars) + 1\n",
    "print('total chars:', vocab_size)\n",
    "char_indices = dict((c, i + 2) for i, c in enumerate(chars))\n",
    "indices_char = dict((i + 2, c) for i, c in enumerate(chars))\n",
    "\n",
    "char_indices['<UNK>'] = 1\n",
    "indices_char[1] = '<UNK>'\n",
    "\n",
    "trainCharData = vectorize_sentences(trainTokensAsString, char_indices, char_maxlen)\n",
    "testCharData = vectorize_sentences(testTokensAsString, char_indices, char_maxlen)\n",
    "\n",
    "trainCharData.shape\n",
    "\n",
    "testCharData.shape\n",
    "\n",
    "char_maxlen\n",
    "\n",
    "cnn_model = create_cnn_model(char_maxlen=char_maxlen, \n",
    "                             vocab_size=vocab_size,\n",
    "                             nb_filter=nb_filter, \n",
    "                             filter_kernels=filter_kernels,\n",
    "                             pool_size=pool_size, \n",
    "                             n_dense_nodes=dense_outputs,\n",
    "                             drop_out=.5, \n",
    "                             n_out=n_out)\n",
    "\n",
    "cnn_res = train_and_test_model(cnn_model, trainCharData[:, :, 1:],\n",
    "                               y_train_labels, \n",
    "                               testCharData[:, :, 1:], \n",
    "                               y_test_labels, \n",
    "                               'cnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_rnn_model(rnn_input_len, char_maxlen, vocab_size,\n",
    "                         embed_matrix, n_RNN_nodes, \n",
    "                         nb_filter=100, filter_kernels = [4] * 4,\n",
    "                         pool_size=3, n_dense_nodes=100,\n",
    "                         recurrent_dropout=0.2, \n",
    "                         drop_out=.2, n_out=2):\n",
    "    \n",
    "    word_input = Input(shape=(rnn_input_len,), name='word_input_layer')\n",
    "    char_input = Input(shape=(char_maxlen, vocab_size), name='char_input_layer')\n",
    "    \n",
    "    word_embeddings = Embedding(input_dim=embed_matrix.shape[0],\n",
    "                                output_dim=embed_matrix.shape[1],\n",
    "                                weights=[embed_matrix], \n",
    "                                mask_zero=True, \n",
    "                                name='word_embedding_layer')(word_input) \n",
    "\n",
    "    rnn_output1 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=True, \n",
    "                                      recurrent_dropout=recurrent_dropout, \n",
    "                                      dropout=drop_out, name='hidden_layer1'))(word_embeddings)\n",
    "    \n",
    "    rnn_output2 = Bidirectional(LSTM(units=n_RNN_nodes, return_sequences=False, \n",
    "                                      recurrent_dropout=recurrent_dropout,\n",
    "                                      dropout=drop_out, name='hidden_layer2'))(rnn_output1)\n",
    "            \n",
    "    conv1 = Conv1D(nb_filter, kernel_size=filter_kernels[0],\n",
    "                  padding='valid', activation='relu',\n",
    "                  input_shape=(char_maxlen, vocab_size))(char_input)\n",
    "\n",
    "    maxpool1 = MaxPool1D(pool_size=pool_size)(conv1)\n",
    "\n",
    "    conv2 = Conv1D(nb_filter, kernel_size=filter_kernels[1],\n",
    "                          padding='valid', activation='relu')(maxpool1)\n",
    "    maxpool2 = MaxPool1D(pool_size=pool_size)(conv2)\n",
    "\n",
    "    conv3 = Conv1D(nb_filter, kernel_size=filter_kernels[2],\n",
    "                          padding='valid', activation='relu')(maxpool2)\n",
    "\n",
    "    conv4 = Conv1D(nb_filter, kernel_size=filter_kernels[3],\n",
    "                          padding='valid', activation='relu')(conv3)\n",
    "\n",
    "    maxpool3 = MaxPool1D(pool_size=pool_size)(conv4)\n",
    "    cnn_output = Flatten()(maxpool3)\n",
    "\n",
    "    merged_layer = concatenate([cnn_output, rnn_output2])\n",
    "    \n",
    "    dense_layer1 = Dense(n_dense_nodes, activation='relu', name='dense_layer')(merged_layer)\n",
    "    drop_out1 = Dropout(drop_out)(dense_layer1)\n",
    "    dense_layer2 = Dense(n_dense_nodes, activation='relu')(drop_out1)\n",
    "    drop_out2 = Dropout(drop_out)(dense_layer2)\n",
    "    \n",
    "    main_output = Dense(n_out, activation='softmax', name='output_layer')(drop_out2)\n",
    "\n",
    "    model = Model(inputs=[word_input, char_input], outputs=[main_output])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", \n",
    "                  metrics=['accuracy', recall, precision])    \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rnn_model = create_cnn_rnn_model(rnn_input_len=train_data.shape[-1], \n",
    "                                     char_maxlen=char_maxlen, \n",
    "                                     vocab_size=vocab_size,\n",
    "                                     embed_matrix=embedding_matrix, \n",
    "                                     n_RNN_nodes=500,\n",
    "                                     nb_filter=nb_filter, \n",
    "                                     filter_kernels=filter_kernels,\n",
    "                                     pool_size=pool_size, \n",
    "                                     n_dense_nodes=400,\n",
    "                                     recurrent_dropout=0.4, \n",
    "                                     drop_out=.5, \n",
    "                                     n_out=n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Show the architecture for model (RNN+CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Report the peformance according to some metric (RNN+CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input_layer (InputLayer)   (None, 1024, 92)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1018, 128)    82560       char_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 203, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 199, 128)     82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 39, 128)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 35, 128)      82048       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "word_input_layer (InputLayer)   (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 33, 128)      49280       conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding_layer (Embedding (None, 100, 100)     1009700     word_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 6, 128)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100, 1000)    2404000     word_embedding_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 768)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000)         6004000     bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1768)         0           flatten_2[0][0]                  \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (Dense)             (None, 400)          707600      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 400)          0           dense_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 400)          160400      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 400)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 2)            802         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,582,438\n",
      "Trainable params: 10,582,438\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 7687 samples, validate on 855 samples\n",
      "Epoch 1/1\n",
      "7687/7687 [==============================] - 65s 8ms/step - loss: 0.7027 - acc: 0.4933 - recall: 0.4932 - precision: 0.4934 - val_loss: 0.6956 - val_acc: 0.4877 - val_recall: 0.4877 - val_precision: 0.4877\n",
      "\n",
      "Epoch 00001: saving model to cnn_rnn_model.hdf5\n",
      "2848/2848 [==============================] - 8s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_rnn_res = train_and_test_model(cnn_rnn_model, \n",
    "                               [train_data, trainCharData[:, :, 1:]],\n",
    "                               y_train_labels, \n",
    "                               [test_data, testCharData[:, :, 1:]],\n",
    "                               y_test_labels, \n",
    "                               'cnn_rnn_model',\n",
    "                               epochs=nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare performance of all of models in a table (precision and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cnn_mod</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn_mod</th>\n",
       "      <td>0.500351</td>\n",
       "      <td>0.500351</td>\n",
       "      <td>0.500351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn_rnn_mod</th>\n",
       "      <td>0.502809</td>\n",
       "      <td>0.502809</td>\n",
       "      <td>0.502809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy    recall  precision\n",
       "cnn_mod      0.500000  0.500000   0.500000\n",
       "rnn_mod      0.500351  0.500351   0.500351\n",
       "cnn_rnn_mod  0.502809  0.502809   0.502809"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records([cnn_res[1:4], rnn_res[1:4], cnn_rnn_res[1:4]], \n",
    "                          columns=['accuracy', 'recall', 'precision'], \n",
    "                         index=['cnn_mod', 'rnn_mod', 'cnn_rnn_mod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifications(classifications, classType, test_y, test_text):\n",
    "    texts = [''.join(sent) for sent in test_text[classifications]]\n",
    "    stock_movements = np.where(test_y[classifications], 'positive', 'negative')\n",
    "    \n",
    "    print('Examples of {} predictions:\\n'.format(classType))\n",
    "    for i in range(len(texts)):\n",
    "        print('Stock movement was {}'.format(stock_movements[i]))\n",
    "        print('News info:\\n{}'.format(texts[i]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print_samples(model, modelName, test_x, test_y=y_test, test_text = X_test):\n",
    "    \"\"\"\"Print out predictions of the model\"\"\"\n",
    "    print('Stats for {} model'.format(modelName))\n",
    "    \n",
    "    res = model.predict(test_x)\n",
    "    class_res = np.apply_along_axis(np.argmax, axis=1, arr=res)\n",
    "\n",
    "    comparisons = class_res == test_y\n",
    "    comparisons = pd.DataFrame(comparisons)\n",
    "    \n",
    "    good_class = comparisons.loc[comparisons[0] == True].index[0:3]\n",
    "    bad_class = comparisons.loc[comparisons[0] == False].index[0:3]\n",
    "\n",
    "    print_classifications(good_class, 'correct', test_y, test_text)\n",
    "    print_classifications(bad_class, 'INcorrect', test_y, test_text)\n",
    "    \n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "    \n",
    "    top3MostProbPosArg = np.argsort(res[:, 1])[-3:]\n",
    "    top3Y = y_test_df.iloc[top3MostProbPosArg]\n",
    "    top3Probs = pd.Series(res[top3MostProbPosArg, 1], index=top3Y.index)\n",
    "    top3Data = pd.concat([top3Y, top3Probs], axis=1)\n",
    "    top3Data.columns = ['Actual', 'PositiveProb']\n",
    "    print('')\n",
    "    print('Top 3 Most Positive Probability:')\n",
    "    print(top3Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Mediobanca CEO says is serene despite probe \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "regulators tell two insurers and GE Capital to upgrade resolution plans \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Lloyds eyes return to private hands in next 12 months \n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Deals of the day- Mergers and acquisitions \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Einhorn sues Apple marks biggest investor challenge in years \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Qatar regulator reprimands RBS for insufficient staff training \n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "      Actual  PositiveProb\n",
      "1144     1.0      0.500726\n",
      "176      0.0      0.501730\n",
      "1874     1.0      0.502338\n",
      "Stats for CNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Mediobanca CEO says is serene despite probe \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "regulators tell two insurers and GE Capital to upgrade resolution plans \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Lloyds eyes return to private hands in next 12 months \n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Deals of the day- Mergers and acquisitions \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Einhorn sues Apple marks biggest investor challenge in years \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Qatar regulator reprimands RBS for insufficient staff training \n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "      Actual  PositiveProb\n",
      "1795     0.0      0.496591\n",
      "260      0.0      0.496991\n",
      "1038     1.0      0.497233\n",
      "Stats for CNN_RNN model\n",
      "Examples of correct predictions:\n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Deals of the day- Mergers and acquisitions \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Einhorn sues Apple marks biggest investor challenge in years \n",
      "\n",
      "Stock movement was positive\n",
      "News info:\n",
      "Qatar regulator reprimands RBS for insufficient staff training \n",
      "\n",
      "Examples of INcorrect predictions:\n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Mediobanca CEO says is serene despite probe \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "regulators tell two insurers and GE Capital to upgrade resolution plans \n",
      "\n",
      "Stock movement was negative\n",
      "News info:\n",
      "Lloyds eyes return to private hands in next 12 months \n",
      "\n",
      "\n",
      "Top 3 Most Positive Probability:\n",
      "      Actual  PositiveProb\n",
      "2086     1.0      0.551061\n",
      "670      0.0      0.552398\n",
      "311      0.0      0.556368\n"
     ]
    }
   ],
   "source": [
    "predict_and_print_samples(rnn_res[0], 'RNN', test_data)\n",
    "\n",
    "predict_and_print_samples(cnn_res[0], 'CNN', testCharData[:, :, 1:])\n",
    "\n",
    "predict_and_print_samples(cnn_rnn_res[0], 'CNN_RNN', [test_data, testCharData[:, :, 1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSERTION for RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of correct predictions:\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### Mediobanca CEO says is serene despite probe \n",
    "\n",
    "#### Here the probe is being made against Mediobanca and we have predicted\n",
    "#### that stock movement is negative which makes sense\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### regulators tell two insurers and GE Capital to upgrade resolution plans \n",
    "\n",
    "#### Here the regulators are against GE Capital and we have predicted\n",
    "#### that stock movement is negative which makes sense\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### Lloyds eyes return to private hands in next 12 months \n",
    "\n",
    "#### Here the Lloyds return to private hands which may allude to existing problems\n",
    "#### and we have predicted that stock movement is negative which makes sense\n",
    "\n",
    "#### Examples of INcorrect predictions:\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Deals of the day- Mergers and acquisitions \n",
    "\n",
    "#### Not every Merger and acquisition can be considered positive\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Einhorn sues Apple marks biggest investor challenge in years \n",
    "\n",
    "#### Here Einhorn sues Apple and we have predicted positive which is incorrect\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Qatar regulator reprimands RBS for insufficient staff training \n",
    "\n",
    "#### Here regulator reprimands RBS and we have predicted positive which is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSERTION for CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of correct predictions:\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### Mediobanca CEO says is serene despite probe \n",
    "\n",
    "#### Here the probe is being made against Mediobanca and we have predicted\n",
    "#### that stock movement is negative which makes sense\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### regulators tell two insurers and GE Capital to upgrade resolution plans \n",
    "\n",
    "#### Here the regulators are against GE Capital and we have predicted\n",
    "#### that stock movement is negative which makes sense\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### Lloyds eyes return to private hands in next 12 months \n",
    "\n",
    "#### Here the Lloyds return to private hands which may allude to existing problems\n",
    "#### and we have predicted that stock movement is negative which makes sense\n",
    "\n",
    "#### Examples of INcorrect predictions:\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Deals of the day- Mergers and acquisitions \n",
    "\n",
    "#### Not every Merger and acquisition can be considered positive\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Einhorn sues Apple marks biggest investor challenge in years \n",
    "\n",
    "#### Here Einhorn sues Apple and we have predicted positive which is incorrect\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Qatar regulator reprimands RBS for insufficient staff training \n",
    "\n",
    "#### Here regulator reprimands RBS and we have predicted positive which is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSERTION for CNN_RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of correct predictions:\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### Mediobanca CEO says is serene despite probe \n",
    "\n",
    "#### Here the probe is being made against Mediobanca and we have predicted\n",
    "#### that stock movement is negative which makes sense\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### regulators tell two insurers and GE Capital to upgrade resolution plans \n",
    "\n",
    "#### Here the regulators are against GE Capital and we have predicted\n",
    "#### that stock movement is negative which makes sense\n",
    "\n",
    "#### Stock movement was negative\n",
    "#### News info:\n",
    "#### Lloyds eyes return to private hands in next 12 months \n",
    "\n",
    "#### Here the Lloyds return to private hands which may allude to existing problems\n",
    "#### and we have predicted that stock movement is negative which makes sense\n",
    "\n",
    "#### Examples of INcorrect predictions:\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Deals of the day- Mergers and acquisitions \n",
    "\n",
    "#### Not every Merger and acquisition can be considered positive\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Einhorn sues Apple marks biggest investor challenge in years \n",
    "\n",
    "#### Here Einhorn sues Apple and we have predicted positive which is incorrect\n",
    "\n",
    "#### Stock movement was positive\n",
    "#### News info:\n",
    "#### Qatar regulator reprimands RBS for insufficient staff training \n",
    "\n",
    "#### Here regulator reprimands RBS and we have predicted positive which is incorrect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
